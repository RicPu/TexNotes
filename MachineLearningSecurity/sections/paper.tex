\section{Paper Analysis}
\subsection{Evaluating the Robustness of "Ensemble Everything Everywhere"}
This paper takes a critical look at a defense against adversarial attacks called "Ensemble Everything Everywhere". This defense was proposed as a promising new way to make image classifiers robust. It works by doing something clever; instead of just looking at an image once, it creates multiple noisy versions of that image at different resolutions, runs them through a neural network, and then combines the internal activations from different layers to make a final, supposedly robust, prediction. The original paper made some bold claims. On the standard benchmarks CIFAR-10 and CIFAR-100, it reported very high robust accuracy against strong adversarial attacks. Perhaps even more compellingly, the authors showed something fascinating: when you attacked their model, the resulting adversarial noise wasn't just random static, it actually looked like the target class. For example, trying to turn a car into a frog would produce perturbations that vaguely resembled frog-like textures. This "perceptually aligned" property is something it is often seen in truly robust models, so it was a strong hint that this defense was on the right track.

However, the authors of this critical paper had a healthy dose of skepticism. The history of adversarial defenses is unfortunately littered with methods that claim high robustness but are later broken, often because they rely on gradient masking (making the model's loss landscape so jagged and unpredictable that gradient-based attacks get lost), not because the model is truly secure. So, they set out to test if "Ensemble Everything Everywhere" was genuinely robust of just another case of gradient masking in disguise.

\paragraph{Problems revealed by the investigation.} Their investigation revealed two main problems:

\begin{enumerate}
    \item \textit{The Defense Relies Heavily on Gradient Obfuscation}: The method's core components (random noise and a special aggregation function called CrossMax) create a highly non-smooth, spiky loss surface. A standard gradient-based attack fails not because the model has true robustness, but because the loss landscape reveals masked gradients.
    \item \textit{The Original Evaluation Wasn't Strong Enough}: The defense was only tested with a standard, off-the-shelf attack suite called AutoAttack. For a defense this complex and randomized, you need a custom, adaptive attack that specifically accounts for its tricks.
\end{enumerate}

\paragraph{Breaking the Defense.} They built a simpler, more effective attack. The key insight was to bypass the problematic parts of the defense during the attack generation. They attacked a source version of the model that replaced the tricky CrossMax aggregation with a simple average, which provided usable gradients. They used a very long 500-step gradient attack and averaged over many random transformations to smooth out the noise. Finally, they simply transferred the adversarial perturbations found on this source model to the original, defended model. The claimed robust accuracy of 72\% on CIFAR-10 plummeted to just 11\%. On CIFAR-100, it fell from 48\% to 14\%. Their adaptive attack achieved a success rate of over 85\%, whereas the previously-used AutoAttack maxed out at around 52\%.

\subsection{Ensemble Everything Everywhere}
The authors show that strong robustness can be achieved without adversarial training. Fine-tuning a pre-trained model on a multi-resolution input stack with a very small learning rate is sufficient to induce robustness. When training from scratch, techniques like mix-up further improve results. Notably, robustness peaks early during training and then decays, suggesting a trade-off between clean accuracy and adversarial robustness that can be tuned via early stopping or learning rate schedulers. The method shows favorable scaling with dataset difficulty: gains on CIFAR-100 are more pronounced than on CIFAR-10. When combined with light adversarial training, performance improves further, achieving significant improvement over previous SOTA\@.

\paragraph{Multi-Resolution Prior.} Inspired by the biological functioning of the human visual system, the multi-resolution prior is a novel training approach designed to enhance a classifier's robustness without explicit adversarial training. The core idea is to present multiple versions of the same image simultaneously, each at a different resolution and then stack these versions channel-wise to create an augmented input. Alongside resolution reduction, each variant is also subjected to controlled augmentations: small random noise, spatial jitter, contrast adjustments, and color-to-grayscale shifts. This mimics how the human eye perceives objects across varying distances, blurs, and lighting conditions, especially through natural eye movements like microsaccades and macrosaccades, which constantly change the retinal image while preserving semantic content. By forcing the model to classify this multi-resolution, multi-perturbed stack as a single input, the network learns a more transformation-invariant representation of objects. This not only improves generalization but also inherently boosts adversarial robustness by reducing the effective attackable input space and making it harder for an adversary to craft perturbations that deceive all resolution layers at once. In experiments, this method yielded measurable gains in robustness on datasets like CIFAR-10 and CIFAR-100, even before any adversarial training was applied.

A central claim of the paper is the Interpretability-Robustness hypothesis: models that produce human-interpretable adversarial examples tend to be more adversarially robust. Under standard attacks, brittle models yield noise-like perturbations that fool the model but are meaningless to humans. In contrast, attacks against the author's multi-resolution models produce semantically meaningful changes. This suggests that robust representation align more closely with human visual perception, and that interpretability can serve as a proxy for robustness.

\begin{equation}
    x_k = \textsc{upscale}\left(\textsc{downscale}(x, d_i) + \mathcal{N}(0, \delta^2_1 \cdot I_{d_i \times d_i})\right) + \mathcal{N}(0, \delta^2_2 \cdot I_{d \times d})
\end{equation}

\paragraph{CrossMax Robust Ensembling.} When ensembling multiple neural networks, the standard approach is to average their output logits. While this improves clean accuracy and uncertainty estimation, it creates a single point of failure in adversarial settings. An attacker can deploy a targeted gradient-based attack against just one vulnerable model in the ensemble. By maximizing that single model's logit for the target class, that outlier prediction can dominate the ensemble's mean, effectively hijacking the entire decision. The authors propose: instead of using the most confident prediction, use the \(k^\text{th}\)-highest confidence per class.

Given logits \(Z\) of shape \NewTexttt{[Batch, N\_models, C\_classes]}:

\begin{enumerate}
    \item \textit{Per-Model Max Subtraction}: Compute \(\hat{Z} = Z - \max(Z, \text{axis}=2)\). This removes each model's intrinsic bias scale. A model that outputs generally higher numbers can no longer dominate. It neutralizes attacks that work by simply boosting all logits of one model.
    \item \textit{Per-Class Max Subtraction}: Compute \(\hat{Z} = \hat{Z} - \max(\hat{Z}, \text{axis}=1)\). This eliminates any single model's extreme confidence for any particular class. Now, for a class to win, it needs consistent high support across multiple models.
    \item \textit{Robust Aggregation}: Finally compute \(Y = \text{median}(\hat{Z}, \text{axis}=1)\). Take the median across models for each class. The median is highly resistant to outliers. An attacker must now manipulate enough models to shift the median, not just create one extreme value.
\end{enumerate}

The paper's most powerful application is creating a self-ensemble from just one model. They discovered that adversarial attacks don't affect all layers of a network equally. An image fooling the final layer might still be correctly identified by the middle or early layers. They attach simple probe classifiers to multiple intermediate layers of a single network. When an input comes in, you get multiple independent predictions. These predictions from different layers of the same model are treated as votes from a committee, and are then aggregated using the CrossMax algorithm. This single-model self-ensemble achieves significant adversarial robustness, because an attack designed to fool the final layer fails to fool the earlier layers, and CrossMax robustly combines these differing opinions.

A key empirical finding is that adversarial susceptibility is not uniform across a network's layers. Attacks crafted to fool the final classification layer do not reliably fool intermediate layers; early and middle layers often retain features of the original class. This layer-wise decorrelation enables the self-ensemble strategy: by aggregating predictions from multiple layers via CrossMax, the model can "notice" when an attack is inconsistent across its own internal representations, thereby resisting manipulation.

\paragraph{Generative Byproduct.} An unexpected outcome of the multi-resolution prior is that it allows pre-trained classifiers and CLIP models to act as controllable image generators. By starting from uniform noise and performing gradient ascent toward a target class using the multi-resolution perturbation parameterization, the optim8izer produces human-interpretable images rather than adversarial noise. This demonstrates that the model's internal representations are semantically structured and that adversarial robustness can coincide with generative interpretability. The multi-resolution attack formulation also serves as a strong transfer prior. The authors successfully craft adversarial images that fool closed-source vision-language models (GPT-4, Gemini, Claude) by optimizing against open-source CLIP variants.


